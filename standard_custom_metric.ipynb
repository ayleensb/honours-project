{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "20f72445-9bfa-4fd5-b2d8-57e79d6a61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.datasets import CompasDataset\n",
    "#import fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fcb9e-fd69-4fc5-aefc-583aab3ecb97",
   "metadata": {},
   "source": [
    "#### 0. COMPAS Dataset used as the starting point to get predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ce438faa-3cd1-48e9-98d6-a58088d4cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 5 rows removed from CompasDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#load COMPAS dataset\n",
    "\n",
    "try:\n",
    "    compas = CompasDataset(\n",
    "        protected_attribute_names=['sex', 'race'],\n",
    "        privileged_classes=[['Female'], ['Caucasian']], \n",
    "        features_to_keep=['age', 'c_charge_degree', 'race', 'age_cat', \n",
    "                          'sex', 'priors_count', 'days_b_screening_arrest', 'c_charge_desc'],\n",
    "        features_to_drop=[],\n",
    "        categorical_features=['age_cat', 'c_charge_degree', 'c_charge_desc'],\n",
    "        label_name='two_year_recid'\n",
    "    )\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    #returns the dataframe and the metadata in a tuple using a function from the compas dataset\n",
    "    df, meta = compas.convert_to_dataframe()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading COMPAS dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb62adb-8e90-4b76-be9d-9be51fa2be62",
   "metadata": {},
   "source": [
    "#### 0.1 Train a model with the COMPAS dataset \n",
    "\n",
    "- 80/20 train/test split\n",
    "- key point is the extraction of the group membership from this dataset to create an array storing group membership for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7dec9e86-adc1-46c5-b165-0c21fa7e9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " group membership:  [1. 1. 0. ... 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy dataset to ensure original remains unchanged\n",
    "df = df.copy()\n",
    "#print(df)\n",
    "\n",
    "#separate features and labels\n",
    "features = ['race', 'sex', 'priors_count', 'c_charge_degree=F', 'c_charge_degree=M']\n",
    "target = 'two_year_recid' #binary target where 0 means does not offend, 1 means offends\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "X_test_indices = X_test.index\n",
    "\n",
    "#retrive each instance's group membership before scaling X_test to make predictions \n",
    "#scaling will make it lose the index information to retrieve this information\n",
    "grp_membership = df.loc[X_test_indices, 'race'].values\n",
    "print(\"\\n group membership: \", grp_membership, \"\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#predicted class labels 0 or 1\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81ef1ad8-6124-4c95-8029-c3602742a97f",
   "metadata": {},
   "source": [
    "#### 1. From the above classifier, extract three arrays that are inputs for new_metric function\n",
    "\n",
    "To do: Have three arrays each for predictions, groud truth labels, and group membership from the dataset and model.\n",
    "\n",
    "Important aspect of this part: indices of each array needs to align such that pred[0] refers to ground_truth[0] and grp_membership[0]. \n",
    "\n",
    "The arrays:\n",
    "1. predictions: positive and negative predictions from the classifier.\n",
    "2. ground_truth: this is the two_yr_recid column that represents the target variables. So, use y_test which contains ground truth values from the train_test_split for the dataset.\n",
    "3. grp_membership: array of group membership for each instance containing privileged (label 1 - Caucasian) and non-privileged (label 0 - not Caucasian) as defined by protected attribute of 'race'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "036a0032-6d6f-456b-b81a-1b6c78a24ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [0. 0. 0. ... 0. 0. 0.]\n",
      "ground truth labels:  [0. 0. 1. ... 0. 1. 1.]\n",
      "group membership:  [1. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#predictions array\n",
    "predictions = y_pred \n",
    "print(\"predictions: \", predictions)\n",
    "\n",
    "#ground truth labels array\n",
    "ground_truth = y_test.values \n",
    "print(\"ground truth labels: \", ground_truth)\n",
    "\n",
    "#group membership array defined above where model is trained.\n",
    "print(\"group membership: \", grp_membership)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4bb2b-1b23-4ed3-9bfa-58f4cf2639f5",
   "metadata": {},
   "source": [
    "#### 2. Function for the new metric using the three arrays as input.\n",
    "\n",
    "- The main conceptual idea behind this metric: as a basic starting point, create a classifier level group metric that takes into consideration individual components.\n",
    "- Assumption: This will allow for a quantification of fairness in a more comprehensive manner. \n",
    "- Evaluate behaviour by: giving the function varied distributions of arrays as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bbdf0f1b-3780-4598-95c9-4f42e53e959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_metric(arr_pred, arr_true, arr_grp): \n",
    "    \n",
    "    #Two arrays for privileged and not privileged \n",
    "    #g1 and g2- contain predictions and trues are lists of lists [[], []]\n",
    "    grp_priv = [[], []]\n",
    "    grp_unpriv = [[], []]\n",
    "\n",
    "    #j is the number of unique groups in arr_grp \n",
    "    j = len(set(arr_grp)) #!an implicit parameter.\n",
    "    \n",
    "    #print(\"total number of unique groups: \", j)\n",
    "\n",
    "    for i, label in enumerate(arr_grp):\n",
    "        #for privileged class\n",
    "        if label == 1.0:\n",
    "            #add the corresponding prediction + gt label for that class using the index associated with that label\n",
    "            grp_priv[0].append(arr_pred[i])\n",
    "            grp_priv[1].append(arr_true[i])\n",
    "        \n",
    "        #for unprivileged class\n",
    "        else:\n",
    "            grp_unpriv[0].append(arr_pred[i])\n",
    "            grp_unpriv[1].append(arr_true[i])\n",
    "    \n",
    "    #print(\"Privileged group: \", grp_priv)\n",
    "    #print(\"Unprivileged group: \", grp_unpriv)\n",
    "    \n",
    "    priv_indiv_bi = [] #stores individual benefit value of each instance\n",
    "    priv_grp_bi = 0 #tracks total benefit for group\n",
    "    \n",
    "    #1. for each index in a group calculate the benefit, bi\n",
    "    for pred, gt in zip(grp_priv[0], grp_priv[1]):\n",
    "        #the individual component from GEI to calculate benefit for each instance in a group\n",
    "        # original bi calculation with original range of 0,1,2\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        \n",
    "        #2. Sum the total benefit of each group\n",
    "        priv_grp_bi += indiv_benefit\n",
    "        \n",
    "        #3. divide by size of group 1 - result of this is for each class\n",
    "        priv_av_bi = priv_grp_bi / len(grp_priv[0]) #this is the total number of instances in each group. [0] has predictions which will give that number\n",
    "\n",
    "        #store individual benefit of each instance in a list\n",
    "        priv_indiv_bi.append(indiv_benefit)\n",
    "\n",
    "    #print(priv_grp_bi)\n",
    "    # print(priv_av_bi)\n",
    "    #print(\"all bi scores for privileged instances:\\n\", priv_indiv_bi)\n",
    "\n",
    "    unpriv_indiv_bi = []\n",
    "    unpriv_grp_bi = 0\n",
    "    for pred, gt in zip(grp_unpriv[0], grp_unpriv[1]):\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        unpriv_grp_bi += indiv_benefit\n",
    "        unpriv_av_bi = unpriv_grp_bi / len(grp_priv[0])\n",
    "        unpriv_indiv_bi.append(indiv_benefit)\n",
    "        \n",
    "    #print(unpriv_grp_bi)\n",
    "    #print(unpriv_av_bi)\n",
    "    #print(\"all bi scores for unprivileged instances:\\n\", unpriv_indiv_bi)\n",
    "\n",
    "    #4. division result is divided by the sum of g1 and g2 - J\n",
    "    result = (priv_av_bi + unpriv_av_bi) / j\n",
    "\n",
    "    #print(\"new metric value: \", result)\n",
    "    #return priv_av_bi, unpriv_av_bi to see individual benefit for each group in arr_grp\n",
    "    return result\n",
    "\n",
    "#print(\"results from COMPAS data classification:\\n\", new_metric(predictions, ground_truth, grp_membership))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c5b58-f6e3-49ec-8398-f54bd4a8e615",
   "metadata": {},
   "source": [
    "#### 3. Simulate classification results for edge cases and expand table with these numbers\n",
    "\n",
    "- to see if simulated classifier does good on one group and not on other (priv vs unpriv groups).\n",
    "- creating arrays with different distrubutions.\n",
    "- wanting to see if my metric gives more intuitive and interpretable results.\n",
    "- test cases where metrics act differently using the fake distributions acting as output from classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffd957-e8d0-4c4a-9ad8-d89611899c4b",
   "metadata": {},
   "source": [
    "#### 3.1 Function that generates different array distributions. \n",
    "\n",
    "##### The ordering flag:\n",
    "- It decides how the instances are arranged in the final array.\n",
    "- It determines whether the array is ordered with all 0s first (ascending) or all 1s first (descending) after the counts have been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0d2fee3b-1702-45aa-b495-7f0fae8fb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group membership array:  [0 0 0 0 0 0 0 0 1 1]\n",
      "group membership with desc feature:  [0 0 0 0 0 0 0 0 1 1]\n",
      "metric score for fixed arrays:  1.75\n",
      "metric score for fixed arrays with group desc flag:  1.75\n",
      "metric score for randomised arrays:  6.0\n"
     ]
    }
   ],
   "source": [
    "#function to generate arrays synthetically by using fixed distributions:\n",
    "\n",
    "#function takes in the size of array and the type of distribution to generate\n",
    "#also takes as input the order for correct 0s and 1s placement.\n",
    "def gen_fixed_dist_combinations(num_of_instances, \n",
    "                                grp_dist, true_dist, pred_dist, \n",
    "                                randomise=False,\n",
    "                                seed=None,\n",
    "                                pred_order='asc', true_order='asc', grp_order='asc'):\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    #a dictionary that stores the types of expected distributions and maps to their corresp probabilties\n",
    "    distribution_mapping = {\"50/50\": (0.5, 0.5), #50% 0s 50% 1s\n",
    "                            \"80/20\": (0.8, 0.2), #80% 0s 20% 1s\n",
    "                            \"90/10\": (0.9, 0.1), #90% 0s 10% 1s\n",
    "                            \"20/80\": (0.2, 0.8), #20% 0s 80% 1s\n",
    "                            \"10/90\":(0.1, 0.9)} #10% 0s 90% 1s\n",
    "\n",
    "    #get the given parameters of probabilties for each distribution from the dict\n",
    "    group_probability = distribution_mapping.get(grp_dist)\n",
    "    gt_probability = distribution_mapping.get(true_dist)\n",
    "    pred_probability = distribution_mapping.get(pred_dist)\n",
    "\n",
    "    if randomise:\n",
    "        arr_grp = np.random.choice([0,1], size=num_of_instances, p=group_probability)\n",
    "        arr_true = np.random.choice([0,1], size=num_of_instances, p=gt_probability)\n",
    "        arr_pred = np.random.choice([0,1], size=num_of_instances, p=pred_probability)\n",
    "\n",
    "    else:\n",
    "        #calculations for fixing the 0s and 1s for each array using probabilities and array size\n",
    "        group_zeroes =  int(num_of_instances * group_probability[0])\n",
    "        group_ones =  num_of_instances - group_zeroes\n",
    "    \n",
    "        true_zeroes = int(num_of_instances * gt_probability[0])\n",
    "        true_ones = num_of_instances - true_zeroes\n",
    "    \n",
    "        pred_zeroes = int(num_of_instances * pred_probability[0])\n",
    "        pred_ones = num_of_instances - pred_zeroes\n",
    "\n",
    "        #create predictions array based on desired order of 0s and 1s.\n",
    "        if pred_order == 'asc':\n",
    "            arr_pred = np.array([0] * pred_zeroes + [1] * pred_ones)\n",
    "        elif pred_order == 'desc':\n",
    "            arr_pred = np.array([1] * pred_zeroes + [0] * pred_ones)\n",
    "        else:\n",
    "            raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "    \n",
    "        #create ground truth labels array based on desired order of 0s and 1s.\n",
    "        if true_order == 'asc':\n",
    "            arr_true = np.array([0] * true_zeroes + [1] * true_ones)\n",
    "        elif pred_order == 'desc':\n",
    "            arr_true = np.array([1] * true_zeroes + [0] * true_ones)\n",
    "        else:\n",
    "            raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "    \n",
    "        #create group memberships array based on desired order of 0s and 1s.\n",
    "        if grp_order == 'asc':\n",
    "            arr_grp = np.array([0] * group_zeroes + [1] * group_ones)\n",
    "        elif grp_order == 'desc':\n",
    "            arr_grp = np.array([1] * group_zeroes + [0] * group_ones)\n",
    "        else:\n",
    "            raise ValueError(\"group membership array order must be 'asc' or 'desc'\")\n",
    "    \n",
    "    return arr_grp, arr_true, arr_pred  \n",
    "\n",
    "\n",
    "\n",
    "arr_grp, arr_true, arr_pred = gen_fixed_dist_combinations(10, \n",
    "                                                          \"80/20\", \n",
    "                                                          \"50/50\", \n",
    "                                                          \"80/20\", \n",
    "                                                          randomise=False,\n",
    "                                                          pred_order='asc', \n",
    "                                                          true_order='asc', \n",
    "                                                          grp_order='asc')\n",
    "\n",
    "arr_grp_desc, arr_true_asc, arr_pred_asc = gen_fixed_dist_combinations(10, \n",
    "                                                          \"80/20\", \n",
    "                                                          \"50/50\", \n",
    "                                                          \"80/20\", \n",
    "                                                          randomise=False,\n",
    "                                                          pred_order='asc', \n",
    "                                                          true_order='asc', \n",
    "                                                          grp_order='asc')\n",
    "\n",
    "arr_grp_rand, arr_true_rand, arr_pred_rand = gen_fixed_dist_combinations(10, \n",
    "                                                          \"80/20\", \n",
    "                                                          \"50/50\", \n",
    "                                                          \"80/20\", \n",
    "                                                          randomise=True)\n",
    "\n",
    "# print(\"predictions array: \", arr_pred)\n",
    "# print(\"grount truth labels array: \", arr_true)\n",
    "print(\"group membership array: \", arr_grp)\n",
    "\n",
    "# #to check if that ordering feature even does anything\n",
    "# print(\"predictions with asc feature: \", arr_pred_asc)\n",
    "# print(\"grount truth labels with asc feature: \", arr_true_asc)\n",
    "print(\"group membership with desc feature: \", arr_grp_desc)\n",
    "\n",
    "# print(\"predictions with randomness: \", arr_pred_rand)\n",
    "# print(\"grount truth labels with randomness: \", arr_true_rand)\n",
    "# print(\"group membership with randomness: \", arr_grp_rand)\n",
    "\n",
    "#checking to see if the desc/asc feature changes the score for the metric:\n",
    "print(\"metric score for fixed arrays: \", new_metric(arr_grp, arr_true, arr_pred))\n",
    "print(\"metric score for fixed arrays with group desc flag: \", new_metric(arr_grp_desc, arr_true_asc, arr_pred_asc))\n",
    "print(\"metric score for randomised arrays: \",new_metric(arr_grp_rand, arr_true_rand, arr_pred_rand))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6924434-8923-4c87-b3d9-0ec38968835a",
   "metadata": {},
   "source": [
    "### Function to calculate the Balanced Accuracy\n",
    "- need the true and predicted pair of arrays\n",
    "- this function takes in two synthetically generated arrays to calculate the BAC score for.\n",
    "- Aim: To have this score as an anchor point to get an idea of positive and negative outcomes from a classifier.\n",
    "- the function is used in the 'automate_analysis' function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d9e8ac14-cd2e-4ec1-be78-e9db669fbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def balanced_accuracy(arr_true, arr_pred):\n",
    "    y_true = arr_true\n",
    "    y_pred = arr_pred\n",
    "    return balanced_accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031bca7-5f40-4de1-af3b-b2dfeec269fa",
   "metadata": {},
   "source": [
    "### Function to return a metric object for AIF360 framework\n",
    "- function returns a metric object that is compatible for the AIF360 framework\n",
    "- the object can be created in the 'automate_analysis' function to then apply AIF360 metrics to.\n",
    "\n",
    "#### Experiment using these metrics: Conducting a comparison of synthetic classification results with evaluation metrics in AIF360 \n",
    "\n",
    "- The goal here is to be able to use the existing metrics from aif360 and get a value for each array distribution.\n",
    "- Outcome: to have a comparison of my metric with existing metrics and be able to argue that the custom metric is somehow more interpretable and than existing ones. Therefore, making it a more comprehensible metric. \n",
    "- Issue: not bring able to give correct inputs to the functions that are needed to apply metrics within the framework.\n",
    "- Possible solution to do later: use the tutorials as example to see how they give input to these functions and perhaps fake the datasets with the fake arrays to give them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a6460ba5-f1dc-4719-8d38-1c87ede48585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset \n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred, seed=42):\n",
    "    \n",
    "    # synthetic feature data just to comply with AIF360 formatting to apply metric.\n",
    "    np.random.seed(seed)\n",
    "    features = pd.DataFrame({\n",
    "        'feature1': np.random.rand(num_of_instances),\n",
    "        'feature2': np.random.rand(num_of_instances),\n",
    "        'race': np.random.randint(0, 2, num_of_instances)  # placeholder protected attribute\n",
    "    })\n",
    "\n",
    "    features['race'] = arr_grp #protected attribute to represent the group membership array\n",
    "    \n",
    "    #these will be the variables to store the generated arrays with varying distributions \n",
    "    #these changing arrays will show the changing score of each metric being applied \n",
    "\n",
    "    data_true = features.copy() #dataframe with true labels\n",
    "    data_true['label'] = arr_true\n",
    "    \n",
    "    data_pred = features.copy() #dataframe with predicted labels\n",
    "    data_pred['label'] = arr_pred\n",
    "    \n",
    "    # Create BinaryLabelDataset objects for true and predicted datasets\n",
    "    dataset_true = BinaryLabelDataset(df=data_true, label_names=['label'], protected_attribute_names=['race'])\n",
    "    dataset_pred = BinaryLabelDataset(df=data_pred, label_names=['label'], protected_attribute_names=['race'])\n",
    "    \n",
    "    privileged_groups = [{'race': 1}]  # represents the majority group\n",
    "    unprivileged_groups = [{'race': 0}]  # represents the minority group\n",
    "    \n",
    "    metric = ClassificationMetric(dataset_true, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    return metric\n",
    "    \n",
    "#aif360_metric = aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e54ff-3146-4f8a-bbc1-dd7c205cd7de",
   "metadata": {},
   "source": [
    "#### Function to automate analysis process\n",
    "\n",
    "- function needs to be able to create different combinations of distributions to give as input to the function that generates arrays.\n",
    "- store the generated arrays in variables\n",
    "- give those variables as input to the custom metrics and store the score in a variable\n",
    "- give pred and true arrays as input to other exisitng metrics and get a score in variables that correspond to each metric.\n",
    "- get BAC similarly\n",
    "- have a list of scores associated with that distribution\n",
    "- add that as a row to the pandas dataframe and export it to latex\n",
    "- use the distribution combination as the naming scheme for the 'approach' that has all the scores associated with it.\n",
    "- returns the dataframe - a table showing the scores for each metric given  arrays with different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e404f116-0696-4b1a-84c8-8066f95f8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    grp_dist true_dist pred_dist  balanced accuracy score  \\\n",
      "0      50/50     50/50     50/50                 1.000000   \n",
      "1      50/50     50/50     80/20                 0.700000   \n",
      "2      50/50     50/50     90/10                 0.600000   \n",
      "3      50/50     50/50     20/80                 0.700000   \n",
      "4      50/50     50/50     10/90                 0.600000   \n",
      "..       ...       ...       ...                      ...   \n",
      "120    10/90     10/90     50/50                 0.777778   \n",
      "121    10/90     10/90     80/20                 0.611111   \n",
      "122    10/90     10/90     90/10                 0.555556   \n",
      "123    10/90     10/90     20/80                 0.944444   \n",
      "124    10/90     10/90     10/90                 1.000000   \n",
      "\n",
      "     standard_custom_metric score  gei_score  statistical_parity_diff  \\\n",
      "0                        1.000000   0.000000                -1.000000   \n",
      "1                        2.500000   0.214286                -0.400000   \n",
      "2                        5.000000   0.333333                -0.200000   \n",
      "3                        0.625000   0.062130                -0.400000   \n",
      "4                        0.555556   0.061224                -0.200000   \n",
      "..                            ...        ...                      ...   \n",
      "120                      1.000000   0.333333                -0.555556   \n",
      "121                      2.500000   1.166667                -0.222222   \n",
      "122                      5.000000   2.000000                -0.111111   \n",
      "123                      0.625000   0.055556                -0.888889   \n",
      "124                      0.555556   0.000000                -1.000000   \n",
      "\n",
      "     disparate_impact  eq_opp_diff  av_odds_diff  \n",
      "0                 0.0          NaN           NaN  \n",
      "1                 0.0          NaN           NaN  \n",
      "2                 0.0          NaN           NaN  \n",
      "3                 0.6          NaN           NaN  \n",
      "4                 0.8          NaN           NaN  \n",
      "..                ...          ...           ...  \n",
      "120               0.0          NaN           NaN  \n",
      "121               0.0          NaN           NaN  \n",
      "122               0.0          NaN           NaN  \n",
      "123               0.0          NaN           NaN  \n",
      "124               0.0          NaN           NaN  \n",
      "\n",
      "[125 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset \n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def automate_analysis(num_of_instances):\n",
    "  \n",
    "    dist_types = [\"50/50\", \"80/20\", \"90/10\", \"20/80\", \"10/90\"]\n",
    "    order_type = [\"asc\", \"desc\"] #for more variation of arrays\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for grp_dist in dist_types:\n",
    "        for true_dist in dist_types:\n",
    "            for pred_dist in dist_types:\n",
    "                #generate array for current combination\n",
    "                arr_grp, arr_true, arr_pred = gen_fixed_dist_combinations(num_of_instances, \n",
    "                                                                          grp_dist, \n",
    "                                                                          true_dist,\n",
    "                                                                          pred_dist)\n",
    "\n",
    "                balanced_accuracy_score = balanced_accuracy(arr_true, arr_pred)\n",
    "                standard_custom_metric = new_metric(arr_grp, arr_true, arr_pred)\n",
    "                \n",
    "                \n",
    "                aif360_metric = aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred, seed=42)\n",
    "                gei_score = aif360_metric.generalized_entropy_index()\n",
    "                statistical_parity_diff = aif360_metric.mean_difference()\n",
    "                disparate_impact = aif360_metric.disparate_impact()\n",
    "                eq_opp_diff =  aif360_metric.equal_opportunity_difference()\n",
    "                av_odds_diff = aif360_metric.average_odds_difference()\n",
    "                \n",
    "                results.append({\n",
    "                    \"grp_dist\":grp_dist,\n",
    "                    \"true_dist\": true_dist,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                    \"balanced accuracy score\": balanced_accuracy_score,\n",
    "                    \"standard_custom_metric score\": standard_custom_metric,                    \n",
    "                    \"gei_score\": gei_score,\n",
    "                    \"statistical_parity_diff\":statistical_parity_diff,\n",
    "                    \"disparate_impact\": disparate_impact,\n",
    "                    \"eq_opp_diff\":eq_opp_diff, \n",
    "                    \"av_odds_diff\":av_odds_diff \n",
    "                })\n",
    "\n",
    "    #create pandas dataframe from the results list\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    return metrics_df\n",
    "\n",
    "num_of_instances = 100\n",
    "metrics_scores_table = automate_analysis(num_of_instances)\n",
    "print(metrics_scores_table)\n",
    "\n",
    "latex_table = metrics_scores_table.to_latex(index=False)\n",
    "#print(latex_table)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "be14e918-de48-4ff0-aed5-0573d829045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays saved to distributions.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['group', 'true', 'pred']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to save each generated array to a numpy zip file:\n",
    "\n",
    "def store_arrays(file, arr_grp, arr_true, arr_pred):\n",
    "    #save three arrays to a .npz file with keys 'group membership', 'ground truth', 'predictions'\n",
    "    np.savez(file, group=arr_grp, true=arr_true, pred=arr_pred)\n",
    "    print(f\"Arrays saved to {file}\")\n",
    "\n",
    "store_arrays(\"distributions.npz\", arr_grp, arr_true, arr_pred)\n",
    "\n",
    "loaded_arrays = np.load('distributions.npz')\n",
    "loaded_arrays.files\n",
    "#loaded_arrays['group']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3bed7-1e4f-4fcf-bf7a-cbea1e0dea21",
   "metadata": {},
   "source": [
    "### Plot the scores being calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00565762-0fda-4c6f-9d79-f7464804dcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
