{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20f72445-9bfa-4fd5-b2d8-57e79d6a61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.datasets import CompasDataset\n",
    "#import fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fcb9e-fd69-4fc5-aefc-583aab3ecb97",
   "metadata": {},
   "source": [
    "#### 0. COMPAS Dataset used as the starting point to get predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce438faa-3cd1-48e9-98d6-a58088d4cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 5 rows removed from CompasDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#load COMPAS dataset\n",
    "\n",
    "try:\n",
    "    compas = CompasDataset(\n",
    "        protected_attribute_names=['sex', 'race'],\n",
    "        privileged_classes=[['Female'], ['Caucasian']], \n",
    "        features_to_keep=['age', 'c_charge_degree', 'race', 'age_cat', \n",
    "                          'sex', 'priors_count', 'days_b_screening_arrest', 'c_charge_desc'],\n",
    "        features_to_drop=[],\n",
    "        categorical_features=['age_cat', 'c_charge_degree', 'c_charge_desc'],\n",
    "        label_name='two_year_recid'\n",
    "    )\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    #returns the dataframe and the metadata in a tuple using a function from the compas dataset\n",
    "    df, meta = compas.convert_to_dataframe()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading COMPAS dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb62adb-8e90-4b76-be9d-9be51fa2be62",
   "metadata": {},
   "source": [
    "#### 0.1 Train a model with the COMPAS dataset \n",
    "\n",
    "- 80/20 train/test split\n",
    "- key point is the extraction of the group membership from this dataset to create an array storing group membership for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7dec9e86-adc1-46c5-b165-0c21fa7e9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " group membership:  [1. 1. 0. ... 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy dataset to ensure original remains unchanged\n",
    "df = df.copy()\n",
    "#print(df)\n",
    "\n",
    "#separate features and labels\n",
    "features = ['race', 'sex', 'priors_count', 'c_charge_degree=F', 'c_charge_degree=M']\n",
    "target = 'two_year_recid' #binary target where 0 means does not offend, 1 means offends\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "X_test_indices = X_test.index\n",
    "\n",
    "#retrive each instance's group membership before scaling X_test to make predictions \n",
    "#scaling will make it lose the index information to retrieve this information\n",
    "grp_membership = df.loc[X_test_indices, 'race'].values\n",
    "print(\"\\n group membership: \", grp_membership, \"\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#predicted class labels 0 or 1\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81ef1ad8-6124-4c95-8029-c3602742a97f",
   "metadata": {},
   "source": [
    "#### 1. From the above classifier, extract three arrays that are inputs for new_metric function\n",
    "\n",
    "To do: Have three arrays each for predictions, groud truth labels, and group membership from the dataset and model.\n",
    "\n",
    "Important aspect of this part: indices of each array needs to align such that pred[0] refers to ground_truth[0] and grp_membership[0]. \n",
    "\n",
    "The arrays:\n",
    "1. predictions: positive and negative predictions from the classifier.\n",
    "2. ground_truth: this is the two_yr_recid column that represents the target variables. So, use y_test which contains ground truth values from the train_test_split for the dataset.\n",
    "3. grp_membership: array of group membership for each instance containing privileged (label 1 - Caucasian) and non-privileged (label 0 - not Caucasian) as defined by protected attribute of 'race'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "036a0032-6d6f-456b-b81a-1b6c78a24ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [0. 0. 0. ... 0. 0. 0.]\n",
      "ground truth labels:  [0. 0. 1. ... 0. 1. 1.]\n",
      "group membership:  [1. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#predictions array\n",
    "predictions = y_pred \n",
    "print(\"predictions: \", predictions)\n",
    "\n",
    "#ground truth labels array\n",
    "ground_truth = y_test.values \n",
    "print(\"ground truth labels: \", ground_truth)\n",
    "\n",
    "#group membership array defined above where model is trained.\n",
    "print(\"group membership: \", grp_membership)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4bb2b-1b23-4ed3-9bfa-58f4cf2639f5",
   "metadata": {},
   "source": [
    "#### 2. Function for the new metric using the three arrays as input.\n",
    "\n",
    "- The main conceptual idea behind this metric: as a basic starting point, create a classifier level group metric that takes into consideration individual components.\n",
    "- Assumption: This will allow for a quantification of fairness in a more comprehensive manner. \n",
    "- Evaluate behaviour by: giving the function varied distributions of arrays as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbdf0f1b-3780-4598-95c9-4f42e53e959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from this classification:  (1.2408759124087592, 0.7372262773722628, 1.7445255474452555)\n"
     ]
    }
   ],
   "source": [
    "def new_metric(arr_pred, arr_true, arr_grp): \n",
    "    \n",
    "    #Two arrays for privileged and not privileged \n",
    "    #g1 and g2- contain predictions and trues are lists of lists [[], []]\n",
    "    grp_priv = [[], []]\n",
    "    grp_unpriv = [[], []]\n",
    "\n",
    "    #j is the number of unique groups in arr_grp \n",
    "    j = len(set(arr_grp)) #!an implicit parameter.\n",
    "    \n",
    "    #print(\"total number of unique groups: \", j)\n",
    "\n",
    "    for i, label in enumerate(arr_grp):\n",
    "        #for privileged class\n",
    "        if label == 1.0:\n",
    "            #add the corresponding prediction + gt label for that class using the index associated with that label\n",
    "            grp_priv[0].append(arr_pred[i])\n",
    "            grp_priv[1].append(arr_true[i])\n",
    "        \n",
    "        #for unprivileged class\n",
    "        else:\n",
    "            grp_unpriv[0].append(arr_pred[i])\n",
    "            grp_unpriv[1].append(arr_true[i])\n",
    "    \n",
    "    #print(\"Privileged group: \", grp_priv)\n",
    "    #print(\"Unprivileged group: \", grp_unpriv)\n",
    "    \n",
    "    priv_indiv_bi = [] #stores individual benefit value of each instance\n",
    "    priv_grp_bi = 0 #tracks total benefit for group\n",
    "    \n",
    "    #1. for each index in a group calculate the benefit, bi\n",
    "    for pred, gt in zip(grp_priv[0], grp_priv[1]):\n",
    "        #the individual component from GEI to calculate benefit for each instance in a group\n",
    "        # original bi calculation with original range of 0,1,2\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        \n",
    "        #2. Sum the total benefit of each group\n",
    "        priv_grp_bi += indiv_benefit\n",
    "        \n",
    "        #3. divide by size of group 1 - result of this is for each class\n",
    "        priv_av_bi = priv_grp_bi / len(grp_priv[0]) #this is the total number of instances in each group. [0] has predictions which will give that number\n",
    "\n",
    "        #store individual benefit of each instance in a list\n",
    "        priv_indiv_bi.append(indiv_benefit)\n",
    "\n",
    "    # print(priv_grp_bi)\n",
    "    # print(priv_av_bi)\n",
    "    # print(priv_indiv_bi)\n",
    "\n",
    "    unpriv_indiv_bi = []\n",
    "    unpriv_grp_bi = 0\n",
    "    for pred, gt in zip(grp_unpriv[0], grp_unpriv[1]):\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        unpriv_grp_bi += indiv_benefit\n",
    "        unpriv_av_bi = unpriv_grp_bi / len(grp_priv[0])\n",
    "        unpriv_indiv_bi.append(indiv_benefit)\n",
    "        \n",
    "    #print(unpriv_grp_bi)\n",
    "    #print(unpriv_av_bi)\n",
    "    #print(unpriv_indiv_bi)\n",
    "\n",
    "    #4. division result is divided by the sum of g1 and g2 - J\n",
    "    result = (priv_av_bi + unpriv_av_bi) / j\n",
    "\n",
    "    #print(\"new metric value: \", result)\n",
    "    return result, priv_av_bi, unpriv_av_bi\n",
    "\n",
    "print(\"results from this classification: \", new_metric(predictions, ground_truth, grp_membership))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c5b58-f6e3-49ec-8398-f54bd4a8e615",
   "metadata": {},
   "source": [
    "#### 3. Simulate classification results for edge cases and expand table with these numbers\n",
    "\n",
    "- to see if simulated classifier does good on one group and not on other (priv vs unpriv groups).\n",
    "- creating arrays with different distrubutions.\n",
    "- wanting to see if my metric gives more intuitive and interpretable results.\n",
    "- test cases where metrics act differently using the fake distributions acting as output from classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffd957-e8d0-4c4a-9ad8-d89611899c4b",
   "metadata": {},
   "source": [
    "#### 3.1 Make a function to generate different array distributions. \n",
    "\n",
    "#### Settings:\n",
    "##### Explanation of swap function:\n",
    "- the tuple of probabilities (0, 1) means [0] is the probability for 0s and [1] for 1s. \n",
    "- If a swap flag is set to True (swap_group=True), the tuple is reversed so that the first element represents the probability for 1.\n",
    "- the effect this has: It changes the interpretation of the distribution tuple as just giving it the distribution e.g. \"80/20\" does not specify 80% 0s or 1s, or 20% 0s or 1s.\n",
    "- For example, the default is set to (0.8, 0.2) meaning 80% 0s and 20% 1s. If swap=True, it becomes 80% 1s and 20% 0s.\n",
    "- This affects how many 0s versus 1s are generated.\n",
    "- Why do this: To see if it impacts metric behaviour.\n",
    "- e.g. if we have an array arr_grp=[1111111100] and arr_pred=[1111100000], arr_true=[1111100000], then the unprivileged group would get true negative outcome.\n",
    "- if arr_grp=[0011111111], then the outcome would switch to true positive. This may have an impact on the metric score. \n",
    "\n",
    "##### How is swap different from order:\n",
    "- It decides how the instances are arranged in the final array.\n",
    "- It determines whether the array is ordered with all 0s first (ascending) or all 1s first (descending) after the counts have been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d2fee3b-1702-45aa-b495-7f0fae8fb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original group probs:  (0.8, 0.2)\n",
      "swapped group prob:  (0.2, 0.8)\n",
      "predictions:  [0 0 0 0 0 1 1 1 1 1]\n",
      "grount truth labels:  [0 0 0 0 0 1 1 1 1 1]\n",
      "group membership:  [0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#function to generate arrays synthetically by using fixed distributions:\n",
    "\n",
    "#function takes in the size of array and the type of distribution to generate\n",
    "#also takes as input a flag to swap the default mapping of probabilities associated with the distribution\n",
    "#also takes as input the order for correct 0s and 1s placement.\n",
    "def gen_dist_combinations(num_of_instances, grp_dist, true_dist, pred_dist, \n",
    "                          pred_order='asc', true_order='asc', grp_order='asc', \n",
    "                          swap_group=False, swap_gt=False, swap_pred=False):\n",
    "    \n",
    "    #a dictionary that stores the types of expected distributions and maps to their corresp probabilties\n",
    "    distribution_mapping = {\"50/50\": (0.5, 0.5), \n",
    "                            \"80/20\": (0.8, 0.2), \n",
    "                            \"90/10\": (0.9, 0.1) }\n",
    "\n",
    "    #get the given parameters of probabilties for each distribution from the dict\n",
    "    group_probability = distribution_mapping.get(grp_dist)\n",
    "    print(\"original group probs: \", group_probability)\n",
    "    gt_probability = distribution_mapping.get(true_dist)\n",
    "    pred_probability = distribution_mapping.get(pred_dist)\n",
    "\n",
    "    #if swap parameter is True, then it means that the first element in the dict is interpreted as being the probability for 1s\n",
    "    if swap_group:\n",
    "        group_probability = (group_probability[1], group_probability[0])\n",
    "        print(\"swapped group prob: \", group_probability)\n",
    "    if swap_gt:\n",
    "        gt_probability = (gt_probability[1], gt_probability[0])\n",
    "    if swap_pred:\n",
    "        pred_probability = (pred_probability[1], pred_probability[0])\n",
    "\n",
    "    #calculations for fixing the 0s and 1s for each array using probabilities and array size\n",
    "    group_zeroes =  int(num_of_instances * group_probability[0])\n",
    "    #print(group_zeroes)\n",
    "    group_ones =  num_of_instances - group_zeroes\n",
    "\n",
    "    true_zeroes = int(num_of_instances * gt_probability[0])\n",
    "    true_ones = num_of_instances - true_zeroes\n",
    "\n",
    "    pred_zeroes = int(num_of_instances * pred_probability[0])\n",
    "    pred_ones = num_of_instances - pred_zeroes\n",
    "\n",
    "    #create predictions array based on desired order of 0s and 1s.\n",
    "    if pred_order == 'asc':\n",
    "        arr_grp = np.array([0] * group_zeroes + [1] * group_ones)\n",
    "    elif pred_order == 'desc':\n",
    "        arr_grp = np.array([1] * group_zeroes + [0] * group_ones)\n",
    "    else:\n",
    "        raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "\n",
    "    #create ground truth labels array based on desired order of 0s and 1s.\n",
    "    if pred_order == 'asc':\n",
    "        arr_grp = np.array([0] * group_zeroes + [1] * group_ones)\n",
    "    elif pred_order == 'desc':\n",
    "        arr_grp = np.array([1] * group_zeroes + [0] * group_ones)\n",
    "    else:\n",
    "        raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "\n",
    "    #create group memberships array based on desired order of 0s and 1s.\n",
    "    if grp_order == 'asc':\n",
    "        arr_grp = np.array([0] * group_zeroes + [1] * group_ones)\n",
    "    elif grp_order == 'desc':\n",
    "        arr_grp = np.array([1] * group_zeroes + [0] * group_ones)\n",
    "    else:\n",
    "        raise ValueError(\"group membership array order must be 'asc' or 'desc'\")\n",
    "    \n",
    "    return arr_grp, arr_true, arr_pred  \n",
    "\n",
    "\n",
    "arr_grp, arr_true, arr_pred = gen_dist_combinations(10, \n",
    "                                                    grp_dist = \"80/20\", \n",
    "                                                    true_dist = \"50/50\", \n",
    "                                                    pred_dist = \"50/50\", \n",
    "                                                    pred_order = \"asc\", \n",
    "                                                    true_order = \"asc\", \n",
    "                                                    grp_order = 'asc', \n",
    "                                                    swap_group=True)\n",
    "print(\"predictions: \", arr_pred)\n",
    "print(\"grount truth labels: \", arr_true)\n",
    "print(\"group membership: \", arr_grp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031bca7-5f40-4de1-af3b-b2dfeec269fa",
   "metadata": {},
   "source": [
    "#### : Conducting a comparison of synthetic classification results with evaluation metrics in AIF360 \n",
    "\n",
    "The goal here is to be able to use the existing metrics from aif360 and get a value for array distributions.\n",
    "\n",
    "Outcome: to have a comparison of my metric with existing metrics and be able to argue that the custom metric is somehow more interpretable and than existing ones. Therefore, making it a more comprehensible metric. \n",
    "\n",
    "Issue: not bring able to give correct inputs to the functions that are needed to apply metrics within the framework.\n",
    "\n",
    "Possible solution to do later: use the tutorials as example to see how they give input to these functions and perhaps fake the datasets with the fake arrays to give them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6460ba5-f1dc-4719-8d38-1c87ede48585",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two datasets are expected to differ only in 'labels' or 'scores'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 32\u001b[0m\n\u001b[0;32m     27\u001b[0m unprivileged_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}] \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#computing metrics \u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m metric \u001b[38;5;241m=\u001b[39m ClassificationMetric(original_dataset, classified_dataset,\n\u001b[0;32m     33\u001b[0m                               unprivileged_groups\u001b[38;5;241m=\u001b[39munprivileged_groups,\n\u001b[0;32m     34\u001b[0m                               privileged_groups\u001b[38;5;241m=\u001b[39mprivileged_groups)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistical Parity Difference:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m.\u001b[39mstatistical_parity_difference())\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisparate Impact:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m.\u001b[39mdisparate_impact())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\metrics\\classification_metric.py:67\u001b[0m, in \u001b[0;36mClassificationMetric.__init__\u001b[1;34m(self, dataset, classified_dataset, unprivileged_groups, privileged_groups)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtemporarily_ignore(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassified_dataset:\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe two datasets are expected to differ only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The two datasets are expected to differ only in 'labels' or 'scores'."
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "#print(df.columns.tolist())\n",
    "#y_test required for gt labels so this is really meant to be the dataset that holds ground truth labels. \n",
    "gt_dataset = BinaryLabelDataset(favorable_label = 1,\n",
    "                                    unfavorable_label = 0,\n",
    "                                    df=df, #do not need to give it the full dataframe because I do not need the full dataset for what I am doing.\n",
    "                                    label_names = ['two_year_recid'],\n",
    "                                    protected_attribute_names=['race'] )\n",
    "\n",
    "\n",
    "# Reconstruct DataFrame with labels\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns) \n",
    "X_test_df['two_year_recid'] = y_test.reset_index(drop=True) #adding actual labels back and resetting the index to align \n",
    "\n",
    "\n",
    "classified_dataset = original_dataset.copy(deepcopy=True)\n",
    "classified_dataset.labels = y_pred.reshape(-1, 1)  # Replace labels with predictions\n",
    "classified_dataset = BinaryLabelDataset(df=pd.DataFrame(X_test_df),\n",
    "                                        label_names=['two_year_recid'],\n",
    "                                        protected_attribute_names=['race'])\n",
    "\n",
    "#assign predictions to the classified dataset after its initialisation\n",
    "classified_dataset.scores = y_pred.reshape(-1,1)\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}] \n",
    "\n",
    "\n",
    "\n",
    "#computing metrics \n",
    "metric = ClassificationMetric(original_dataset, classified_dataset,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
