{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "20f72445-9bfa-4fd5-b2d8-57e79d6a61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.datasets import CompasDataset\n",
    "#import fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fcb9e-fd69-4fc5-aefc-583aab3ecb97",
   "metadata": {},
   "source": [
    "#### 0. COMPAS Dataset used as the starting point to get predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ce438faa-3cd1-48e9-98d6-a58088d4cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 5 rows removed from CompasDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#load COMPAS dataset\n",
    "\n",
    "try:\n",
    "    compas = CompasDataset(\n",
    "        protected_attribute_names=['sex', 'race'],\n",
    "        privileged_classes=[['Female'], ['Caucasian']], \n",
    "        features_to_keep=['age', 'c_charge_degree', 'race', 'age_cat', \n",
    "                          'sex', 'priors_count', 'days_b_screening_arrest', 'c_charge_desc'],\n",
    "        features_to_drop=[],\n",
    "        categorical_features=['age_cat', 'c_charge_degree', 'c_charge_desc'],\n",
    "        label_name='two_year_recid'\n",
    "    )\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    #returns the dataframe and the metadata in a tuple using a function from the compas dataset\n",
    "    df, meta = compas.convert_to_dataframe()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading COMPAS dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb62adb-8e90-4b76-be9d-9be51fa2be62",
   "metadata": {},
   "source": [
    "#### 0.1 Train a model with the COMPAS dataset \n",
    "\n",
    "- 80/20 train/test split\n",
    "- key point is the extraction of the group membership from this dataset to create an array storing group membership for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7dec9e86-adc1-46c5-b165-0c21fa7e9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " group membership:  [1. 1. 0. ... 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy dataset to ensure original remains unchanged\n",
    "df = df.copy()\n",
    "#print(df)\n",
    "\n",
    "#separate features and labels\n",
    "features = ['race', 'sex', 'priors_count', 'c_charge_degree=F', 'c_charge_degree=M']\n",
    "target = 'two_year_recid' #binary target where 0 means does not offend, 1 means offends\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "X_test_indices = X_test.index\n",
    "\n",
    "#retrive each instance's group membership before scaling X_test to make predictions \n",
    "#scaling will make it lose the index information to retrieve this information\n",
    "grp_membership = df.loc[X_test_indices, 'race'].values\n",
    "print(\"\\n group membership: \", grp_membership, \"\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#predicted class labels 0 or 1\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81ef1ad8-6124-4c95-8029-c3602742a97f",
   "metadata": {},
   "source": [
    "#### 1. From the above classifier, extract three arrays that are inputs for new_metric function\n",
    "\n",
    "To do: Have three arrays each for predictions, groud truth labels, and group membership from the dataset and model.\n",
    "\n",
    "Important aspect of this part: indices of each array needs to align such that pred[0] refers to ground_truth[0] and grp_membership[0]. \n",
    "\n",
    "The arrays:\n",
    "1. predictions: positive and negative predictions from the classifier.\n",
    "2. ground_truth: this is the two_yr_recid column that represents the target variables. So, use y_test which contains ground truth values from the train_test_split for the dataset.\n",
    "3. grp_membership: array of group membership for each instance containing privileged (label 1 - Caucasian) and non-privileged (label 0 - not Caucasian) as defined by protected attribute of 'race'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "036a0032-6d6f-456b-b81a-1b6c78a24ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [0. 0. 0. ... 0. 0. 0.]\n",
      "ground truth labels:  [0. 0. 1. ... 0. 1. 1.]\n",
      "group membership:  [1. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#predictions array\n",
    "predictions = y_pred \n",
    "print(\"predictions: \", predictions)\n",
    "\n",
    "#ground truth labels array\n",
    "ground_truth = y_test.values \n",
    "print(\"ground truth labels: \", ground_truth)\n",
    "\n",
    "#group membership array defined above where model is trained.\n",
    "print(\"group membership: \", grp_membership)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4bb2b-1b23-4ed3-9bfa-58f4cf2639f5",
   "metadata": {},
   "source": [
    "#### 2. Function for the new metric using the three arrays as input.\n",
    "\n",
    "- The main conceptual idea behind this metric: as a basic starting point, create a classifier level group metric that takes into consideration individual components.\n",
    "- Assumption: This will allow for a quantification of fairness in a more comprehensive manner. \n",
    "- Evaluate behaviour by: giving the function varied distributions of arrays as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bbdf0f1b-3780-4598-95c9-4f42e53e959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_metric(arr_pred, arr_true, arr_grp): \n",
    "    \n",
    "    #Two arrays for privileged and not privileged \n",
    "    #g1 and g2- contain predictions and trues are lists of lists [[], []]\n",
    "    grp_priv = [[], []]\n",
    "    grp_unpriv = [[], []]\n",
    "\n",
    "    #j is the number of unique groups in arr_grp \n",
    "    j = len(set(arr_grp)) #!an implicit parameter.\n",
    "    \n",
    "    #print(\"total number of unique groups: \", j)\n",
    "\n",
    "    for i, label in enumerate(arr_grp):\n",
    "        #for privileged class\n",
    "        if label == 1.0:\n",
    "            #add the corresponding prediction + gt label for that class using the index associated with that label\n",
    "            grp_priv[0].append(arr_pred[i])\n",
    "            grp_priv[1].append(arr_true[i])\n",
    "        \n",
    "        #for unprivileged class\n",
    "        else:\n",
    "            grp_unpriv[0].append(arr_pred[i])\n",
    "            grp_unpriv[1].append(arr_true[i])\n",
    "    \n",
    "    #print(\"Privileged group: \", grp_priv)\n",
    "    #print(\"Unprivileged group: \", grp_unpriv)\n",
    "    \n",
    "    priv_indiv_bi = [] #stores individual benefit value of each instance\n",
    "    priv_grp_bi = 0 #tracks total benefit for group\n",
    "    \n",
    "    #1. for each index in a group calculate the benefit, bi\n",
    "    for pred, gt in zip(grp_priv[0], grp_priv[1]):\n",
    "        #the individual component from GEI to calculate benefit for each instance in a group\n",
    "        # original bi calculation with original range of 0,1,2\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        \n",
    "        #2. Sum the total benefit of each group\n",
    "        priv_grp_bi += indiv_benefit\n",
    "        \n",
    "        #3. divide by size of group 1 - result of this is for each class\n",
    "        priv_av_bi = priv_grp_bi / len(grp_priv[0]) #this is the total number of instances in each group. [0] has predictions which will give that number\n",
    "\n",
    "        #store individual benefit of each instance in a list\n",
    "        priv_indiv_bi.append(indiv_benefit)\n",
    "\n",
    "    #print(priv_grp_bi)\n",
    "    # print(priv_av_bi)\n",
    "    #print(\"all bi scores for privileged instances:\\n\", priv_indiv_bi)\n",
    "\n",
    "    unpriv_indiv_bi = []\n",
    "    unpriv_grp_bi = 0\n",
    "    for pred, gt in zip(grp_unpriv[0], grp_unpriv[1]):\n",
    "        indiv_benefit = (int(pred) - int(gt)) + 1\n",
    "        unpriv_grp_bi += indiv_benefit\n",
    "        unpriv_av_bi = unpriv_grp_bi / len(grp_priv[0])\n",
    "        unpriv_indiv_bi.append(indiv_benefit)\n",
    "        \n",
    "    #print(unpriv_grp_bi)\n",
    "    #print(unpriv_av_bi)\n",
    "    #print(\"all bi scores for unprivileged instances:\\n\", unpriv_indiv_bi)\n",
    "\n",
    "    #4. division result is divided by the sum of g1 and g2 - J\n",
    "    result = (priv_av_bi + unpriv_av_bi) / j\n",
    "\n",
    "    #print(\"new metric value: \", result)\n",
    "    #return priv_av_bi, unpriv_av_bi to see individual benefit for each group in arr_grp\n",
    "    return result\n",
    "\n",
    "#print(\"results from COMPAS data classification:\\n\", new_metric(predictions, ground_truth, grp_membership))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf1b14-f845-4b15-abc0-d7d55c8c0270",
   "metadata": {},
   "source": [
    "#### Below are test cases being given as input to the metric function.\n",
    "\n",
    "- it returns a score of the final calculation result = (priv_av_bi + unpriv_av_bi) / j\n",
    "- this is an average of the average bi values for priviliged and unprivileged groups of the protected label.\n",
    "- Also printing the list of individual bi scores for each instance in the group array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1844df10-fbd0-4267-88c2-21347891dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from test case 1 (random classifier) - all arrays with 50/50:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# test case 1: arr_grp1_asc, arr_true1, arr_pred1 \n",
    "print(\"results from test case 1 (random classifier) - all arrays with 50/50:\\n\", new_metric(arr_grp1_asc, arr_true1, arr_pred1) )\n",
    "\n",
    "# # test case 2: arr_grp1_desc, arr_true1_asc, arr_pred1_asc \n",
    "# print(\"results from test case 2 - all arrays with 50/50 but arr_grp has 1s before 0s:\\n\", new_metric(arr_grp1_desc, arr_true1_asc, arr_pred1_asc))\n",
    "\n",
    "# # test case 3: arr_grp2, arr_true2, arr_pred2 \n",
    "# print(\"results from test case 3 - arr_true is 80/20, rest are 50/50:\\n\", new_metric(arr_grp2, arr_true2, arr_pred2) )\n",
    "\n",
    "# # test case 4: arr_grp3, arr_true3, arr_pred3 \n",
    "# print(\"results from test case 4 - arr_pred is 80/20, rest are 50/50:\\n\", new_metric(arr_grp3, arr_true3, arr_pred3))\n",
    "\n",
    "# #test case 5:\n",
    "# print(\"results from test case 5:\\n\", new_metric(arr_grp4, arr_true4, arr_pred4) )\n",
    "\n",
    "# #test case 6:\n",
    "# print(\"results from test case 6:\\n\", new_metric(arr_grp5, arr_true5, arr_pred5) )\n",
    "\n",
    "# #test case 7:\n",
    "# print(\"results from test case 7:\\n\", new_metric(arr_grp6, arr_true6, arr_pred6) )\n",
    "\n",
    "# #test case 8: \n",
    "# print(\"results from test case 8 :\\n\", new_metric(arr_grp7, arr_true7, arr_pred7) )\n",
    "\n",
    "# #test case 9:\n",
    "# print(\"results from test case 9 :\\n\", new_metric(arr_grp8, arr_true8, arr_pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c5b58-f6e3-49ec-8398-f54bd4a8e615",
   "metadata": {},
   "source": [
    "#### 3. Simulate classification results for edge cases and expand table with these numbers\n",
    "\n",
    "- to see if simulated classifier does good on one group and not on other (priv vs unpriv groups).\n",
    "- creating arrays with different distrubutions.\n",
    "- wanting to see if my metric gives more intuitive and interpretable results.\n",
    "- test cases where metrics act differently using the fake distributions acting as output from classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffd957-e8d0-4c4a-9ad8-d89611899c4b",
   "metadata": {},
   "source": [
    "#### 3.1 Make a function to generate different array distributions. \n",
    "\n",
    "#### Settings:\n",
    "##### Explanation of swap function:\n",
    "- the tuple of probabilities (0, 1) means [0] is the probability for 0s and [1] for 1s. \n",
    "- If a swap flag is set to True (swap_group=True), the tuple is reversed so that the first element represents the probability for 1.\n",
    "- the effect this has: It changes the interpretation of the distribution tuple as just giving it the distribution e.g. \"80/20\" does not specify 80% 0s or 1s, or 20% 0s or 1s.\n",
    "- For example, the default is set to (0.8, 0.2) meaning 80% 0s and 20% 1s. If swap=True, it becomes 80% 1s and 20% 0s.\n",
    "- This affects how many 0s versus 1s are generated.\n",
    "- Why do this: To see if it impacts metric behaviour.\n",
    "- e.g. if we have an array arr_grp=[1111111100] and arr_pred=[1111100000], arr_true=[1111100000], then the unprivileged group would get true negative outcome.\n",
    "- if arr_grp=[0011111111], then the outcome would switch to true positive. This may have an impact on the metric score. \n",
    "\n",
    "##### How is swap different from order:\n",
    "- It decides how the instances are arranged in the final array.\n",
    "- It determines whether the array is ordered with all 0s first (ascending) or all 1s first (descending) after the counts have been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0d2fee3b-1702-45aa-b495-7f0fae8fb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate arrays synthetically by using fixed distributions:\n",
    "\n",
    "#function takes in the size of array and the type of distribution to generate\n",
    "#also takes as input a flag to swap the default mapping of probabilities associated with the distribution\n",
    "#also takes as input the order for correct 0s and 1s placement.\n",
    "def gen_fixed_dist_combinations(num_of_instances, grp_dist, true_dist, pred_dist, \n",
    "                          pred_order='asc', true_order='asc', grp_order='asc', \n",
    "                          swap_group=False, swap_gt=False, swap_pred=False):\n",
    "    \n",
    "    #a dictionary that stores the types of expected distributions and maps to their corresp probabilties\n",
    "    distribution_mapping = {\"50/50\": (0.5, 0.5), \n",
    "                            \"80/20\": (0.8, 0.2), \n",
    "                            \"90/10\": (0.9, 0.1), \n",
    "                            \"20/80\": (0.2, 0.8), \n",
    "                            \"10/90\":(0.1, 0.9)}\n",
    "\n",
    "    #get the given parameters of probabilties for each distribution from the dict\n",
    "    group_probability = distribution_mapping.get(grp_dist)\n",
    "    #print(\"original group probs: \", group_probability)\n",
    "    gt_probability = distribution_mapping.get(true_dist)\n",
    "    pred_probability = distribution_mapping.get(pred_dist)\n",
    "   \n",
    "\n",
    "    #if swap parameter is True, then it means that the first element in the dict is interpreted as being the probability for 1s\n",
    "    # if swap_group:\n",
    "    #     group_probability = (group_probability[1], group_probability[0])\n",
    "    #     #print(\"swapped group prob: \", group_probability)\n",
    "    # if swap_gt:\n",
    "    #     gt_probability = (gt_probability[1], gt_probability[0])\n",
    "    # if swap_pred:\n",
    "    #     pred_probability = (pred_probability[1], pred_probability[0])\n",
    "\n",
    "    #calculations for fixing the 0s and 1s for each array using probabilities and array size\n",
    "    group_zeroes =  int(num_of_instances * group_probability[0])\n",
    "    group_ones =  num_of_instances - group_zeroes\n",
    "\n",
    "    true_zeroes = int(num_of_instances * gt_probability[0])\n",
    "    true_ones = num_of_instances - true_zeroes\n",
    "\n",
    "    pred_zeroes = int(num_of_instances * pred_probability[0])\n",
    "    pred_ones = num_of_instances - pred_zeroes\n",
    "\n",
    "    #create predictions array based on desired order of 0s and 1s.\n",
    "    if pred_order == 'asc':\n",
    "        arr_pred = np.array([0] * pred_zeroes + [1] * pred_ones)\n",
    "    elif pred_order == 'desc':\n",
    "        arr_pred = np.array([1] * pred_zeroes + [0] * pred_ones)\n",
    "    else:\n",
    "        raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "\n",
    "    #create ground truth labels array based on desired order of 0s and 1s.\n",
    "    if true_order == 'asc':\n",
    "        arr_true = np.array([0] * true_zeroes + [1] * true_ones)\n",
    "    elif pred_order == 'desc':\n",
    "        arr_true = np.array([1] * true_zeroes + [0] * true_ones)\n",
    "    else:\n",
    "        raise ValueError(\"prediction array order must be 'asc' or 'desc'\")\n",
    "\n",
    "    #create group memberships array based on desired order of 0s and 1s.\n",
    "    if grp_order == 'asc':\n",
    "        arr_grp = np.array([0] * group_zeroes + [1] * group_ones)\n",
    "    elif grp_order == 'desc':\n",
    "        arr_grp = np.array([1] * group_zeroes + [0] * group_ones)\n",
    "    else:\n",
    "        raise ValueError(\"group membership array order must be 'asc' or 'desc'\")\n",
    "    \n",
    "    return arr_grp, arr_true, arr_pred  \n",
    "\n",
    "# print(\"predictions: \", arr_pred)\n",
    "# print(\"grount truth labels: \", arr_true)\n",
    "# print(\"group membership is 50/50: \", arr_grp_asc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6924434-8923-4c87-b3d9-0ec38968835a",
   "metadata": {},
   "source": [
    "### Function to calculate the Balanced Accuracy of each array\n",
    "- need the true and predicted pair of arrays\n",
    "- Aim: To have this score as an anchor point to get an idea of positive and negative outcomes from a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d9e8ac14-cd2e-4ec1-be78-e9db669fbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def balanced_accuracy(arr_true, arr_pred):\n",
    "    y_true = arr_true\n",
    "    y_pred = arr_pred\n",
    "    return balanced_accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031bca7-5f40-4de1-af3b-b2dfeec269fa",
   "metadata": {},
   "source": [
    "### Function to return a metric object for AIF360 framework\n",
    "- function returns a metric object that is compatible for the AIF360 framework\n",
    "- the object can be created in the 'automate_analysis' function to then apply AIF360 metrics to.\n",
    "\n",
    "#### Experiment using these metrics: Conducting a comparison of synthetic classification results with evaluation metrics in AIF360 \n",
    "\n",
    "- The goal here is to be able to use the existing metrics from aif360 and get a value for each array distribution.\n",
    "- Outcome: to have a comparison of my metric with existing metrics and be able to argue that the custom metric is somehow more interpretable and than existing ones. Therefore, making it a more comprehensible metric. \n",
    "- Issue: not bring able to give correct inputs to the functions that are needed to apply metrics within the framework.\n",
    "- Possible solution to do later: use the tutorials as example to see how they give input to these functions and perhaps fake the datasets with the fake arrays to give them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a6460ba5-f1dc-4719-8d38-1c87ede48585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset \n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred, seed=42):\n",
    "    \n",
    "    # synthetic feature data just to comply with AIF360 formatting to apply metric.\n",
    "    np.random.seed(seed)\n",
    "    features = pd.DataFrame({\n",
    "        'feature1': np.random.rand(num_of_instances),\n",
    "        'feature2': np.random.rand(num_of_instances),\n",
    "        'race': np.random.randint(0, 2, num_of_instances)  # placeholder protected attribute\n",
    "    })\n",
    "\n",
    "    features['race'] = arr_grp #protected attribute to represent the group membership array\n",
    "    \n",
    "    #these will be the variables to store the generated arrays with varying distributions \n",
    "    #these changing arrays will show the changing score of each metric being applied \n",
    "\n",
    "    data_true = features.copy() #dataframe with true labels\n",
    "    data_true['label'] = arr_true\n",
    "    \n",
    "    data_pred = features.copy() #dataframe with predicted labels\n",
    "    data_pred['label'] = arr_pred\n",
    "    \n",
    "    # Create BinaryLabelDataset objects for true and predicted datasets\n",
    "    dataset_true = BinaryLabelDataset(df=data_true, label_names=['label'], protected_attribute_names=['race'])\n",
    "    dataset_pred = BinaryLabelDataset(df=data_pred, label_names=['label'], protected_attribute_names=['race'])\n",
    "    \n",
    "    privileged_groups = [{'race': 1}]  # represents the majority group\n",
    "    unprivileged_groups = [{'race': 0}]  # represents the minority group\n",
    "    \n",
    "    metric = ClassificationMetric(dataset_true, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    return metric\n",
    "    \n",
    "#aif360_metric = aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e54ff-3146-4f8a-bbc1-dd7c205cd7de",
   "metadata": {},
   "source": [
    "#### Function to automate analysis process\n",
    "\n",
    "- function needs to be able to create different combinations of distributions to give as input to the function that generates arrays.\n",
    "- store the generated arrays in variables\n",
    "- give those variables as input to the custom metrics and store the score in a variable\n",
    "- give pred and true arrays as input to other exisitng metrics and get a score in variables that correspond to each metric.\n",
    "- get BAC similarly\n",
    "- have a list of scores associated with that distribution\n",
    "- add that as a row to the pandas dataframe and export it to latex\n",
    "- use the distribution combination as the naming scheme for the 'approach' that has all the scores associated with it.\n",
    "- returns the dataframe - a table showing the scores for each metric given  arrays with different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e404f116-0696-4b1a-84c8-8066f95f8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    grp_dist true_dist pred_dist  balanced accuracy score  \\\n",
      "0      50/50     50/50     50/50                 1.000000   \n",
      "1      50/50     50/50     80/20                 0.700000   \n",
      "2      50/50     50/50     90/10                 0.600000   \n",
      "3      50/50     50/50     20/80                 0.700000   \n",
      "4      50/50     50/50     10/90                 0.600000   \n",
      "..       ...       ...       ...                      ...   \n",
      "120    10/90     10/90     50/50                 0.777778   \n",
      "121    10/90     10/90     80/20                 0.611111   \n",
      "122    10/90     10/90     90/10                 0.555556   \n",
      "123    10/90     10/90     20/80                 0.944444   \n",
      "124    10/90     10/90     10/90                 1.000000   \n",
      "\n",
      "     standard_custom_metric score  gei_score  statistical_parity_diff  \\\n",
      "0                        1.000000   0.000000                -1.000000   \n",
      "1                        2.500000   0.214286                -0.400000   \n",
      "2                        5.000000   0.333333                -0.200000   \n",
      "3                        0.625000   0.062130                -0.400000   \n",
      "4                        0.555556   0.061224                -0.200000   \n",
      "..                            ...        ...                      ...   \n",
      "120                      1.000000   0.333333                -0.555556   \n",
      "121                      2.500000   1.166667                -0.222222   \n",
      "122                      5.000000   2.000000                -0.111111   \n",
      "123                      0.625000   0.055556                -0.888889   \n",
      "124                      0.555556   0.000000                -1.000000   \n",
      "\n",
      "     disparate_impact  eq_opp_diff  av_odds_diff  \n",
      "0                 0.0          NaN           NaN  \n",
      "1                 0.0          NaN           NaN  \n",
      "2                 0.0          NaN           NaN  \n",
      "3                 0.6          NaN           NaN  \n",
      "4                 0.8          NaN           NaN  \n",
      "..                ...          ...           ...  \n",
      "120               0.0          NaN           NaN  \n",
      "121               0.0          NaN           NaN  \n",
      "122               0.0          NaN           NaN  \n",
      "123               0.0          NaN           NaN  \n",
      "124               0.0          NaN           NaN  \n",
      "\n",
      "[125 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset \n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def automate_analysis(num_of_instances):\n",
    "  \n",
    "    dist_types = [\"50/50\", \"80/20\", \"90/10\", \"20/80\", \"10/90\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for grp_dist in dist_types:\n",
    "        for true_dist in dist_types:\n",
    "            for pred_dist in dist_types:\n",
    "                #generate array for current combination\n",
    "                arr_grp, arr_true, arr_pred = gen_fixed_dist_combinations(num_of_instances, \n",
    "                                                                          grp_dist, \n",
    "                                                                          true_dist,\n",
    "                                                                          pred_dist)\n",
    "\n",
    "                balanced_accuracy_score = balanced_accuracy(arr_true, arr_pred)\n",
    "                standard_custom_metric = new_metric(arr_grp, arr_true, arr_pred)\n",
    "                \n",
    "                \n",
    "                aif360_metric = aif360_metric_object(num_of_instances, arr_grp, arr_true, arr_pred, seed=42)\n",
    "                gei_score = aif360_metric.generalized_entropy_index()\n",
    "                statistical_parity_diff = aif360_metric.mean_difference()\n",
    "                disparate_impact = aif360_metric.disparate_impact()\n",
    "                eq_opp_diff =  aif360_metric.equal_opportunity_difference()\n",
    "                av_odds_diff = aif360_metric.average_odds_difference()\n",
    "                \n",
    "                results.append({\n",
    "                    \"grp_dist\":grp_dist,\n",
    "                    \"true_dist\": true_dist,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                    \"balanced accuracy score\": balanced_accuracy_score,\n",
    "                    \"standard_custom_metric score\": standard_custom_metric,                    \n",
    "                    \"gei_score\": gei_score,\n",
    "                    \"statistical_parity_diff\":statistical_parity_diff,\n",
    "                    \"disparate_impact\": disparate_impact,\n",
    "                    \"eq_opp_diff\":eq_opp_diff, \n",
    "                    \"av_odds_diff\":av_odds_diff \n",
    "                })\n",
    "\n",
    "    #create pandas dataframe from the results list\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    return metrics_df\n",
    "\n",
    "num_of_instances = 100\n",
    "metrics_scores_table = automate_analysis(num_of_instances)\n",
    "print(metrics_scores_table)\n",
    "\n",
    "latex_table = metrics_scores_table.to_latex(index=False)\n",
    "#print(latex_table)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "be14e918-de48-4ff0-aed5-0573d829045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays saved to distributions.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['group', 'true', 'pred']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to save each generated array to a numpy zip file:\n",
    "\n",
    "def store_arrays(file, arr_grp, arr_true, arr_pred):\n",
    "    #save three arrays to a .npz file with keys 'group membership', 'ground truth', 'predictions'\n",
    "    np.savez(file, group=arr_grp, true=arr_true, pred=arr_pred)\n",
    "    print(f\"Arrays saved to {file}\")\n",
    "\n",
    "store_arrays(\"distributions.npz\", arr_grp, arr_true, arr_pred)\n",
    "\n",
    "loaded_arrays = np.load('distributions.npz')\n",
    "loaded_arrays.files\n",
    "#loaded_arrays['group']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3bed7-1e4f-4fcf-bf7a-cbea1e0dea21",
   "metadata": {},
   "source": [
    "### Plot the scores being calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00565762-0fda-4c6f-9d79-f7464804dcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
